<p align="center">
  <img src="https://img.shields.io/badge/Python-3.8+-3776AB?style=for-the-badge&logo=python&logoColor=white" alt="Python"/>
  <img src="https://img.shields.io/badge/LangChain-RAG-1C3C3C?style=for-the-badge&logo=langchain&logoColor=white" alt="LangChain"/>
  <img src="https://img.shields.io/badge/FastAPI-009688?style=for-the-badge&logo=fastapi&logoColor=white" alt="FastAPI"/>
  <img src="https://img.shields.io/badge/Qdrant-DC382D?style=for-the-badge&logo=qdrant&logoColor=white" alt="Qdrant"/>
</p>

# ğŸ¥ Medical RAG QA â€” Meditron 7B LLM

> A **Retrieval-Augmented Generation (RAG)** powered medical question-answering system that uses **Meditron 7B** LLM, **Qdrant** vector database, and **PubMedBERT** embeddings to deliver accurate, document-grounded answers to medical and oncology questions.

---

## ğŸ“Œ Table of Contents

- [Overview](#-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Tech Stack](#-tech-stack)
- [Project Structure](#-project-structure)
- [Getting Started](#-getting-started)
- [Usage](#-usage)
- [How It Works](#-how-it-works)
- [Configuration](#%EF%B8%8F-configuration)
- [Future Improvements](#-future-improvements)
- [License](#-license)

---

## ğŸ” Overview

Standard LLMs often **hallucinate** medical facts, producing confident but incorrect information â€” a serious risk in healthcare. This project solves that problem by implementing a **RAG pipeline** that grounds every answer in **real medical PDF documents**.

Users ask medical questions through a web-based chat interface, and the system retrieves relevant context from a vector database of ingested medical documents before generating an answer using a **locally-running, privacy-preserving medical LLM**.

### Why This Approach?

| Problem | Solution |
|---|---|
| LLMs hallucinate medical facts | RAG grounds answers in **real medical documents** |
| Medical data is in unstructured PDFs | Pipeline **extracts, chunks, and vectorizes** PDF content |
| Cloud APIs are expensive & raise privacy concerns | **Locally-running open-source LLM** (Meditron 7B via GGUF) |
| General embeddings miss medical semantics | **PubMedBERT** embeddings trained on biomedical literature |
| No user-friendly interface | **Web-based chat UI** via FastAPI + Jinja2 + Bootstrap 5 |

---

## âœ¨ Key Features

- ğŸ§  **Medical-domain LLM** â€” Meditron 7B, fine-tuned on clinical guidelines & PubMed data
- ğŸ”’ **Privacy-first** â€” Runs entirely locally, no data sent to external APIs
- ğŸ“„ **PDF knowledge base** â€” Ingest any medical PDF as a knowledge source
- âš¡ **CPU inference** â€” Quantized GGUF model runs without a GPU
- ğŸ¯ **Source attribution** â€” Every answer includes the source document and context
- ğŸ–¥ï¸ **Web chat UI** â€” Dark-themed, responsive interface built with Bootstrap 5
- ğŸ” **Semantic search** â€” PubMedBERT embeddings for biomedical-aware retrieval
- ğŸ—„ï¸ **Production-grade vector DB** â€” Qdrant for fast, scalable similarity search

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     USER (Browser)                       â”‚
â”‚              http://localhost:8000                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚  HTTP POST /get_response
                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FastAPI Server (rag.py)                 â”‚
â”‚                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Jinja2 UI   â”‚   â”‚ Prompt       â”‚   â”‚ RetrievalQA  â”‚  â”‚
â”‚  â”‚ (index.html) â”‚   â”‚ Template     â”‚   â”‚   Chain       â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                              â”‚           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚                       â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â”‚
          â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Qdrant Vector   â”‚â—„â”€â”€â”€ query â”€â”€â”‚  PubMedBERT          â”‚
â”‚  Database        â”‚   embedding  â”‚  Embeddings          â”‚
â”‚  (localhost:6333)â”‚              â”‚  (NeuML/pubmedbert)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â”‚  top-k context
          â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Meditron 7B LLM (GGUF, local)               â”‚
â”‚          Loaded via CTransformers (llama type)           â”‚
â”‚                                                          â”‚
â”‚  Config: max_tokens=1024, temp=0.1, top_k=50, top_p=0.9 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â”‚
          â–¼
    Generated Answer + Source Document â†’ Browser
```

---

## ğŸ›  Tech Stack

### Core AI/ML

| Technology | Role |
|---|---|
| **[Meditron 7B](https://huggingface.co/epfl-llm/meditron-7b)** | Medical LLM (GGUF Q4_K_M quantized) for answer generation |
| **[PubMedBERT](https://huggingface.co/NeuML/pubmedbert-base-embeddings)** | Biomedical embedding model for text vectorization |
| **[CTransformers](https://github.com/marella/ctransformers)** | CPU-optimized runtime for GGUF quantized models |
| **[LangChain](https://www.langchain.com/)** | RAG orchestration framework (RetrievalQA chain) |

### Data & Storage

| Technology | Role |
|---|---|
| **[Qdrant](https://qdrant.tech/)** | High-performance vector database for embeddings |
| **PyPDFLoader** | PDF text extraction from medical documents |
| **RecursiveCharacterTextSplitter** | Document chunking with overlap for context preservation |

### Web & API

| Technology | Role |
|---|---|
| **[FastAPI](https://fastapi.tiangolo.com/)** | Async Python web framework |
| **Jinja2** | Server-side HTML templating |
| **Bootstrap 5** | Responsive dark-themed UI |
| **Uvicorn** | ASGI server |

---

## ğŸ“ Project Structure

```
Medical-RAG-using-Meditron-7B-LLM/
â”‚
â”œâ”€â”€ Data/                           # Medical PDF knowledge base
â”‚   â”œâ”€â”€ cancer_and_cure__a_critical_analysis.27.pdf
â”‚   â””â”€â”€ medical_oncology_handbook_june_2020_edition.pdf
â”‚
â”œâ”€â”€ templates/
â”‚   â””â”€â”€ index.html                  # Web chat interface (Bootstrap 5)
â”‚
â”œâ”€â”€ ingest.py                       # Step 1: PDF ingestion â†’ Vector DB
â”œâ”€â”€ rag.py                          # Step 2: FastAPI server + RAG pipeline
â”œâ”€â”€ retriever.py                    # Utility: standalone retrieval testing
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ LICENSE                         # MIT License
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### Prerequisites

- **Python 3.8+**
- **Docker** (for Qdrant) or Qdrant installed locally
- **~4 GB disk space** for the quantized model file

### 1. Clone the Repository

```bash
git clone https://github.com/<your-username>/Medical-RAG-using-Meditron-7B-LLM.git
cd Medical-RAG-using-Meditron-7B-LLM
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Download the Meditron 7B Model

Download the quantized GGUF model file from Hugging Face and place it in the project root:

```bash
# Download meditron-7b.Q4_K_M.gguf from:
# https://huggingface.co/TheBloke/meditron-7B-GGUF
```

### 4. Start Qdrant Vector Database

```bash
docker run -p 6333:6333 qdrant/qdrant
```

### 5. Ingest Medical Documents

```bash
python ingest.py
```

This reads all PDFs from `Data/`, generates PubMedBERT embeddings, and stores them in Qdrant.

### 6. Run the Application

```bash
uvicorn rag:app --reload
```

Open **http://localhost:8000** in your browser.

---

## ğŸ’¬ Usage

1. Open the web interface at `http://localhost:8000`
2. Type a medical question in the text area (e.g., _"What is Metastatic disease?"_)
3. Click **Submit**
4. The system returns:
   - âœ… **Answer** â€” Generated by Meditron 7B, grounded in your documents
   - ğŸ“„ **Source Context** â€” The exact document chunk used for the answer
   - ğŸ“ **Source Document** â€” The PDF file the information came from

---

## âš™ How It Works

The application operates in **two phases**:

### Phase 1 â€” Document Ingestion (`ingest.py`)

```
Medical PDFs (Data/)
    â†’ Load with PyPDFLoader
    â†’ Split into chunks (1000 chars, 100 char overlap)
    â†’ Generate embeddings with PubMedBERT
    â†’ Store vectors in Qdrant DB
```

- **Chunk size of 1000** balances context richness with retrieval precision
- **100-character overlap** prevents information loss at chunk boundaries
- **PubMedBERT** embeddings capture biomedical semantic relationships (e.g., _"neoplasm" â‰ˆ "tumor"_)

### Phase 2 â€” Query & Answer (`rag.py`)

```
User Question (Web UI)
    â†’ Embed question with PubMedBERT
    â†’ Similarity search in Qdrant (top-1 result)
    â†’ Retrieved context + question â†’ Prompt Template
    â†’ Meditron 7B generates answer
    â†’ Return answer + source to UI
```

- **Low temperature (0.1)** ensures deterministic, factual responses
- Prompt explicitly instructs the model to **not hallucinate** â€” if the answer isn't in the context, it says "I don't know"
- **Source attribution** enables users to verify every answer

---

## âš™ï¸ Configuration

The LLM configuration in `rag.py` can be tuned:

| Parameter | Default | Description |
|---|---|---|
| `max_new_tokens` | 1024 | Maximum tokens in generated response |
| `context_length` | 2048 | Context window size |
| `temperature` | 0.1 | Lower = more deterministic (recommended for medical) |
| `top_k` | 50 | Top-K sampling parameter |
| `top_p` | 0.9 | Nucleus sampling parameter |
| `repetition_penalty` | 1.1 | Penalizes repeated tokens |
| `threads` | CPU cores / 2 | Number of inference threads |

---
